{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from unidecode import unidecode\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams, FreqDist\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "\n",
    "genius = lyricsgenius.Genius(\"8C8ZjZRD9R1-ftPYBpLYAFya4dO71sB_6Q1ZJ5bgnLuTpxzqQl84NA9mW0HRJrIr\")\n",
    "\n",
    "genius.verbose = False # Turn off status messages\n",
    "genius.remove_section_headers = True # Remove section headers (e.g. [Chorus]) from lyrics when searching\n",
    "genius.skip_non_songs = False # Include hits thought to be non-songs (e.g. track lists)\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\"] # Exclude songs with these words in their title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = genius.search_artist(\"Eminem\", max_songs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics_Eminem.txt already exists. Overwrite?\n",
      "(y/n): y\n",
      "Wrote `Lyrics_Eminem.txt`\n"
     ]
    }
   ],
   "source": [
    "artist.save_lyrics(extension='txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing data\n",
    "dataset = open(\"Lyrics_Eminem.txt\", 'r+')\n",
    "train_data = dataset.read().lower().replace(',',' ').replace('.',' ').replace('?',' ').replace('!',' ').replace(':',' ').replace(';',' ').replace('   ',' ').replace('/',' ').split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I uploaded one song \"Stan\" used as test data\n",
    "dataset2 = open(\"test_song.txt\", 'r+')\n",
    "test_data = dataset2.read().lower().replace(',',' ').replace('.',' ').replace('?',' ').replace('!',' ').replace(':',' ').replace(';',' ').replace('   ',' ').replace('/',' ').split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unigram\n",
    "1. smoothing method: add one\n",
    "2. In order to minimize the errors, when calculate perplexity, parameters converted into logarithm and then accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "unigramsDist = FreqDist()  # uni-gram词频数字典\n",
    "for i in train_data:\n",
    "    sWordFreq = FreqDist(word_tokenize(i))  # 每一句的词频数字典\n",
    "    for j in sWordFreq:\n",
    "        if j in unigramsDist:\n",
    "            unigramsDist[j] += sWordFreq[j]\n",
    "        else:\n",
    "            unigramsDist[j] = sWordFreq[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# add words which not occur in test data set\n",
    "for i in test_data:\n",
    "    word = word_tokenize(i)  # dictionary (word frequency in each sentence)\n",
    "    for j in word:\n",
    "        if j not in unigramsDist:\n",
    "            unigramsDist[j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count to frequency  smoothing: add one # unigramsDist.B()\n",
    "s = unigramsDist.N() + unigramsDist.B()\n",
    "unigramsFreq = FreqDist()\n",
    "for i in unigramsDist:\n",
    "    unigramsFreq[i] = (unigramsDist[i] + 1) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt = []\n",
    "for sentence in test_data:\n",
    "    logprob = 0\n",
    "    wt = 0\n",
    "    for word in word_tokenize(sentence):\n",
    "        if word in unigramsFreq:\n",
    "            logprob += log(unigramsFreq[word],2)\n",
    "            wt += 1\n",
    "    if wt > 0:\n",
    "        ppt.append([sentence,pow(2,-(logprob/wt))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of unigram is: 504.0672176830519\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "for i in ppt:\n",
    "    temp += i[1]\n",
    "print(\"The perplexity of unigram is:\", temp/len(ppt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigram\n",
    "(same method as unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2gram = {}     # all the number of 2-gram type words begin with 'w' \n",
    "bigramsDist = FreqDist()\n",
    "\n",
    "for sentence in train_data:\n",
    "    sWordFreq = FreqDist(bigrams(word_tokenize(sentence)))\n",
    "    for j in sWordFreq:\n",
    "        if j in bigramsDist:\n",
    "            bigramsDist[j] += sWordFreq[j]\n",
    "        else:\n",
    "            bigramsDist[j] = sWordFreq[j]\n",
    "            if j[0] in w2gram:\n",
    "                w2gram[j[0]] += 1\n",
    "            else:\n",
    "                w2gram[j[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test part\n",
    "for sentence in test_data:\n",
    "    word = bigrams(word_tokenize(sentence))\n",
    "    for j in word:\n",
    "        if j not in bigramsDist:\n",
    "            bigramsDist[j] = 0\n",
    "            \n",
    "            if j[0] in w2gram:\n",
    "                w2gram[j[0]] += 1\n",
    "            else:\n",
    "                w2gram[j[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {}\n",
    "for i in bigramsDist:\n",
    "    if i[0] in history:\n",
    "        history[i[0]] += bigramsDist[i]\n",
    "    else:\n",
    "        history[i[0]] = bigramsDist[i]\n",
    "bigramsFreq = FreqDist()\n",
    "for i in bigramsDist:\n",
    "    bigramsFreq[i] = (bigramsDist[i] + 1) / (history[i[0]] + w2gram[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt2 = []\n",
    "for sentence in test_data:\n",
    "    logprob = 0\n",
    "    wt = 0\n",
    "    for word in bigrams(word_tokenize(sentence)):\n",
    "        if word in bigramsFreq:\n",
    "            logprob += log(bigramsFreq[word],2)\n",
    "            wt += 1\n",
    "    if wt > 0:\n",
    "        ppt2.append([sentence,pow(2,-(logprob/wt))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of bigram is: 13.312246352914487\n"
     ]
    }
   ],
   "source": [
    "temp2 = 0\n",
    "for i in ppt2:\n",
    "    temp2 += i[1]\n",
    "print(\"The perplexity of bigram is:\", temp2/len(ppt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conlusion\n",
    "As the result of the comparation of two models(unigram and bigram) with same smoothing method(add one), we can see that the perplexity of unigram is much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_eminem = genius.search_artist(\"Eminem\", max_songs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 30 songs lyrics \n",
    "lyrics_lib = []\n",
    "for i in artist_eminem.songs:\n",
    "    lyrics_lib.append(i.lyrics)\n",
    "\n",
    "data=pd.DataFrame(lyrics_lib,columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length is: 137672\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for index, row in data['text'].iteritems():\n",
    "    cleaned = str(row).lower().replace(' ', '\\n')\n",
    "    text = text + \" \".join(re.findall(r\"[a-z']+\", cleaned))\n",
    "print('total length is:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 28\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r\"[a-z'\\s]\", text)\n",
    "\n",
    "chars = sorted(list(set(tokens)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 45878\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               80384     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                3612      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28)                0         \n",
      "=================================================================\n",
      "Total params: 83,996\n",
      "Trainable params: 83,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45878/45878 [==============================] - 51s 1ms/step - loss: 1.1107\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t like tree sap i don't hate trap and i \"\n",
      "t like tree sap i don't hate trap and i don't fall in the fuck watch when i don't see we are just come take it but your can call me and i can face tang a cread we call me no mar ir come tank and i trink it say i'm still bely get your gursed in the balls ind well you seem the muck wath we have in the call me want shady to say to the miss your chance to the balls on of you like it was me then i take it the bad in the real slim shady yes i\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t like tree sap i don't hate trap and i \"\n",
      "t like tree sap i don't hate trap and i don't see un a firtin' to see with the rider with the versut out here to the muck's say i'm half you i don't do bagged like the lighty balk and i'm stall like it's wasn and everything that i ain't gonna bound and i don't wanna smoke and you ind i think it feels stupp streman she's not so bad 'cause i never lose your and i'm bronive and i'm prayled to again eath other the fuck in this minst that sh\n",
      "Epoch 2/10\n",
      "45878/45878 [==============================] - 44s 967us/step - loss: 1.0904\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"t what was i just about to say oh yeah l\"\n",
      "t what was i just about to say oh yeah like i'm the best to get a play the world get fucked just the way it warn of the moment of a trade i this shit is i don't want to as i can and i tasking a this monster and i got the world gr she late and shoot your lacks and i don't want to ya the bad i don't get stim the moment of a trade to be the best the way it warn your seem the moment of a trade i'm a this to the bad i feel i got the world ge\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"t what was i just about to say oh yeah l\"\n",
      "t what was i just about to say oh yeah like put a hap in the morthing i got up in a tered me but i don't mare of the begroest a mon and i'm not afraid in my face me on the begest some better never resentle cranged the compan purkin' like a fucked bit is scap to fell of stop and so this shit i wanna son' love me with a chan it take me like a winders when it's too my fag and i can face on everyone fack fack on word and i'm a duce not i wa\n",
      "Epoch 3/10\n",
      "45878/45878 [==============================] - 44s 953us/step - loss: 1.0756\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"s screamin' l l l let us in l l l like a\"\n",
      "s screamin' l l l let us in l l l like a amagable i'm a dicause i love the with the more that i'm the first we don't the mornams i don't slipp and shady play they say i'm the fuck i ain't gonna smoke my but a this i don't slip a cont and say i don't slipp and i can case the bad for the way that you're bitter i love the way they say i'm the flow i done with the real shady play the but i wante that you're beit i singed that the though is \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"s screamin' l l l let us in l l l like a\"\n",
      "s screamin' l l l let us in l l l like a sings they parks to walk the deam and i can't see me the chollars i know i'l the dan't even calle stand to say that you think i'm crazy it say i'ma have to smy everythings i ain't slyep a are for the side i was mon the though the move my parkla seem and i every way the spand the world greatest in the world greatest in the with the down i ain't gonna do i let me got right no mord so i don't slipp \n",
      "Epoch 4/10\n",
      "45878/45878 [==============================] - 44s 952us/step - loss: 1.0622\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" it's in my grasp but as soon as i grab \"\n",
      " it's in my grasp but as soon as i grab but it i'm beand i'm not knowin' me and i'm not knowin' of you starts to my back and i'm gone to say to me to say i groud to see i need so everything i got a rubort i'm a thing and scary on a to the mise the shit's side i'm passel i'm gonna rele bit of all the shit's be and i think i'm sick at it i got a fuckin' this i know that i'm side i'm sookion the moment to the back it so be the same realize\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" it's in my grasp but as soon as i grab \"\n",
      " it's in my grasp but as soon as i grab but your come to the world get on the beat like you off it's my jobics the fuck it we cause it was to say the nem and i'm gone to say you don't feel like i ain't the fack of sucfels and i ond you maning all they get in my back in the fuck i ain't no jee s on the minith and i can be all the sappolest the tonggury that i got the sting it and and i'm not knowin' for but i slip i ain't gon't want it t\n",
      "Epoch 5/10\n",
      "45878/45878 [==============================] - 44s 954us/step - loss: 1.0425\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"motherfucker like the last motherfucker \"\n",
      "motherfucker like the last motherfucker say that you can't all it all i don't fall on my back and i can't she dear to ask in my bast in the back like a fuckin' starts to all the mation i can't see you ain't eakin' so baby don't all the best to see what i was bee all the bling to all the mation i say the wall that shit back in the moment when i was but you to shet i'm sired i can't see at me to see what i have to say to get your chance t\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"motherfucker like the last motherfucker \"\n",
      "motherfucker like the last motherfucker say that me but left and i high this moment when it before i love me to sold on my comestin' me that it's still to say that marning around there the way to see all the real slim shady please stand up and when it was me this is the thoughts i hate to be the bad feel me but i got to be the best all i was but he call maybed i was badn' and i'm sidy with that me and i'll get that shit gonna slowin' so\n",
      "Epoch 6/10\n",
      "45878/45878 [==============================] - 46s 1ms/step - loss: 1.0375\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"be in my genes i got a laptop in my back\"\n",
      "be in my genes i got a laptop in my back i got the trung well that you could try in a poshin' the bar got a chanch thing back to dear the trung before i'm a thing i don't slim shady all a all and screamin' my fack on everyone fack fack on everyone yoke and i told your dame a follorin' my feet a fuckin' screamin' and i can tell your dammy wrotter what i get the way that you ain't gonna do i don't do what i get the chairstand a chance to \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"be in my genes i got a laptop in my back\"\n",
      "be in my genes i got a laptop in my back on the way that you to see a playin' i hate to fack on my fack on my fack it i'm straid and say i hate to be the blow about of drowntenge and the grankt i might a chant they get bit of a thie s and say i feel your tange standing the tenemiss sayin' anymay' and i'm gray and i'm gone to take a flompted of just and was and played and so the minst to fack on my fack in the chap you say i get your mon\n",
      "Epoch 7/10\n",
      "45878/45878 [==============================] - 44s 964us/step - loss: 1.0255\n",
      "----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ble big trouble and if he is as bananas \"\n",
      "ble big trouble and if he is as bananas the straights simestit's back to show you can sack and you buine and this shit my lift to be the best the world greatestes and if you could never say i could just but i got the couple stands i can't even shit it and i could was but i got that i was ter this is the rap like a brool and i can feel it and if i walk on waitens but i got some comem not they say i'm sidevaser and it to the motherfuckers\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ble big trouble and if he is as bananas \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ble big trouble and if he is as bananas the shit it's too bad knowly get it all and you can and newreally while i go see what they from the dreat and i can case out to feel your stim i realed all that i shit it's too still into everyobooty hardge to see worse where i was bah you and you can s not this mar 'ear as a tand the rap they was to bround it and you but 'til the reatter i don't stup a poround it my but i sourd to me from me the \n",
      "Epoch 8/10\n",
      "45878/45878 [==============================] - 42s 920us/step - loss: 1.0173\n",
      "----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"asy on you not to hurt your feelings but\"\n",
      "asy on you not to hurt your feelings but i'm a firt to be the real slim shady shout out and i can't she dead to be can't shut her hanger and so it the bat i ain't no jee shite in my back in that you don't slike to make an everybody gooball the moment i'm the bad fuck the shout to go get sound the shone to feel like to do was the more to my feet me like an my fines when it to say this shit to say this shit that i got some to get your and\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"asy on you not to hurt your feelings but\"\n",
      "asy on you not to hurt your feelings but i slie she deered i spot the slund shout to go she rock called be my comentin' that miss you to say you through the stants don't fall on my fan clown and grow and i can't she dead to take the back like a warnan you can sitce rick one there back in one ho way 'cause you're gonna gad to be the real slim shady slesply you the latter to you live me these sliep with a chapter i'm sookin' to fack on me\n",
      "Epoch 9/10\n",
      "45878/45878 [==============================] - 43s 930us/step - loss: 1.0069\n",
      "----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" for broke so everybody everybody go ber\"\n",
      " for broke so everybody everybody go berate like a stand the bad is the bad what's when i was back to the world greate to know that i say what's when i was bet and never do it the colute the couple stiting to and say i got a fuckin' like i wanna sightunaness without the sting and say i got that's why it's back to that i have to get to get the shit that's why i'm still time i gring the matter and so i have to get the shit that's when i w\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" for broke so everybody everybody go ber\"\n",
      " for broke so everybody everybody go berate like a mighter but i sururd i only go back of you got that's how much we wainabaca to that you daddy woo sound my back to that mindtinges and paill and i can't be the real slimis and that's when i hate the back like s one up in the real slim shady playe i'm god the crews who she lone who we arjond me with some don't mare that's how much we we we waytes contross i feel like i wanter this minnic\n",
      "Epoch 10/10\n",
      "45878/45878 [==============================] - 43s 928us/step - loss: 0.9987\n",
      "----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"thers oh what a tangled web we have caus\"\n",
      "thers oh what a tangled web we have cause up with a chance back up in this moment like a moman when i'm gone the stantin' to ask i take a ponther what i have to be the best to see with a chaich that's when i may bane mate about as i have to be the really so that i say that as i high as go one the way that you'll be one mommant to the minuth and the couple stand the thought of bed and i take a couple stand up what i think and i could not\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"thers oh what a tangled web we have caus\"\n",
      "thers oh what a tangled web we have cause i got a chanchtyou're gonna rould go the way to wanna stard and to that i said here the back lilk a fuckin' been a colliv i'm sinde and nom he armon a stand the real slim shady yeah i said here man and they so i hate the moment of me the way that you set the way that i'm sobody come nom and i don't know what he gett ling just the shit that you're marhe one house i got a warturted in a tout a tak\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "history = model.fit(\n",
    "    x, \n",
    "    y,\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    callbacks=[print_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a475f0c10>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output():\n",
    "    generated = ''\n",
    "    usr_input = input(\"Write down anything you want: \")\n",
    "\n",
    "    sentence = ('{0:0>' + str(Tx) + '}').format(usr_input).lower()\n",
    "    generated += usr_input \n",
    "\n",
    "    sys.stdout.write(\"\\n\\nHere is your fake Eminem song: \\n\\n\") \n",
    "    sys.stdout.write(usr_input)\n",
    "    for i in range(1000):\n",
    "\n",
    "        x_pred = np.zeros((1, Tx, len(chars)))\n",
    "\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char != '0':\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature = 0.2)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if next_char == '\\n':\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write down anything you want: love the way you lie\n",
      "\n",
      "\n",
      "Here is your fake Eminem song: \n",
      "\n",
      "love the way you lie i love the way to let you hate to go we how me hold your can't shead and i take a couple stand up what i the couple when i'm gone could 'bout to go and i coul a tanged that shit i got a chanchts what i the matter i have to be the best to see with this mounk of nove of you massed me off all i don't know what he get to be the matter and so that i'm scarin' to say you can't see up and i can't see up and the couple stand the world get me one when i was not so bad in on my fath c moman call on my fatter that's where the shill on the same really so that i say that you have the way that i'm right i hate to the man but i got a chanchty as a roll you was just i got a chance back in this man that i can't see up and so that you'll be and i come me that i say that i the real slim shady please stand up but i got a chanchty call my lift as with a chaster and to that i can't see up and sound the way that you can't shealy up and the greatest when i'm gonna said your pain think i got a way the way tha"
     ]
    }
   ],
   "source": [
    "Tx = 40\n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "1. Loss value decreased gradually with learning process\n",
    "2. In general, the lyrics written by the text generator are still not human-like. \n",
    "   Possible reasons such as not enough learning data(I only use 30 songs, but that is maxium for my conputer to train), epoch also is too small, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
